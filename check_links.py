#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç½‘ç«™é“¾æ¥æ£€æŸ¥å·¥å…·
æ£€æŸ¥æ‰€æœ‰HTMLæ–‡ä»¶ä¸­çš„å¤–éƒ¨é“¾æ¥å¯è®¿é—®æ€§
"""

import os
import re
import requests
import time
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
import json

class LinkChecker:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.timeout = 10
        self.results = {
            'accessible': [],
            'inaccessible': [],
            'errors': []
        }
        
    def extract_links_from_html(self, file_path):
        """ä»HTMLæ–‡ä»¶ä¸­æå–æ‰€æœ‰å¤–éƒ¨é“¾æ¥"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # åŒ¹é…hrefå±æ€§ä¸­çš„é“¾æ¥
            href_pattern = r'href=["\'](https?://[^"\']+)["\']'
            href_links = re.findall(href_pattern, content)
            
            # åŒ¹é…srcå±æ€§ä¸­çš„é“¾æ¥
            src_pattern = r'src=["\'](https?://[^"\']+)["\']'
            src_links = re.findall(src_pattern, content)
            
            all_links = list(set(href_links + src_links))
            return all_links
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
            return []
    
    def check_link(self, url):
        """æ£€æŸ¥å•ä¸ªé“¾æ¥çš„å¯è®¿é—®æ€§"""
        try:
            response = self.session.head(url, timeout=self.timeout, allow_redirects=True)
            if response.status_code < 400:
                return {'url': url, 'status': 'accessible', 'code': response.status_code}
            else:
                return {'url': url, 'status': 'inaccessible', 'code': response.status_code}
        except requests.exceptions.RequestException as e:
            return {'url': url, 'status': 'error', 'error': str(e)}
    
    def check_all_links(self, directory='.'):
        """æ£€æŸ¥ç›®å½•ä¸­æ‰€æœ‰HTMLæ–‡ä»¶çš„é“¾æ¥"""
        html_files = []
        
        # æŸ¥æ‰¾æ‰€æœ‰HTMLæ–‡ä»¶
        for root, dirs, files in os.walk(directory):
            for file in files:
                if file.endswith('.html'):
                    html_files.append(os.path.join(root, file))
        
        print(f"æ‰¾åˆ° {len(html_files)} ä¸ªHTMLæ–‡ä»¶")
        
        all_links = set()
        file_links = {}
        
        # æå–æ‰€æœ‰é“¾æ¥
        for file_path in html_files:
            links = self.extract_links_from_html(file_path)
            file_links[file_path] = links
            all_links.update(links)
        
        print(f"æ‰¾åˆ° {len(all_links)} ä¸ªå”¯ä¸€çš„å¤–éƒ¨é“¾æ¥")
        
        # æ£€æŸ¥é“¾æ¥å¯è®¿é—®æ€§
        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_url = {executor.submit(self.check_link, url): url for url in all_links}
            
            for future in as_completed(future_to_url):
                result = future.result()
                if result['status'] == 'accessible':
                    self.results['accessible'].append(result)
                elif result['status'] == 'inaccessible':
                    self.results['inaccessible'].append(result)
                else:
                    self.results['errors'].append(result)
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report(file_links)
        
    def generate_report(self, file_links):
        """ç”Ÿæˆæ£€æŸ¥æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("é“¾æ¥æ£€æŸ¥æŠ¥å‘Š")
        print("="*60)
        
        print(f"\nâœ… å¯è®¿é—®çš„é“¾æ¥ ({len(self.results['accessible'])}):")
        for link in self.results['accessible']:
            print(f"  - {link['url']} (çŠ¶æ€ç : {link['code']})")
        
        print(f"\nâŒ ä¸å¯è®¿é—®çš„é“¾æ¥ ({len(self.results['inaccessible'])}):")
        for link in self.results['inaccessible']:
            print(f"  - {link['url']} (çŠ¶æ€ç : {link['code']})")
        
        print(f"\nâš ï¸  æ£€æŸ¥å‡ºé”™çš„é“¾æ¥ ({len(self.results['errors'])}):")
        for link in self.results['errors']:
            print(f"  - {link['url']} (é”™è¯¯: {link['error']})")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_data = {
            'summary': {
                'total_links': len(self.results['accessible']) + len(self.results['inaccessible']) + len(self.results['errors']),
                'accessible': len(self.results['accessible']),
                'inaccessible': len(self.results['inaccessible']),
                'errors': len(self.results['errors'])
            },
            'file_links': file_links,
            'results': self.results
        }
        
        with open('link_check_report.json', 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: link_check_report.json")
        
        # ç”Ÿæˆéœ€è¦ä¿®å¤çš„é“¾æ¥åˆ—è¡¨
        broken_links = [link['url'] for link in self.results['inaccessible'] + self.results['errors']]
        if broken_links:
            print(f"\nğŸ”§ éœ€è¦ä¿®å¤çš„é“¾æ¥:")
            for link in broken_links:
                print(f"  - {link}")
            
            # ä¿å­˜éœ€è¦ä¿®å¤çš„é“¾æ¥
            with open('broken_links.txt', 'w', encoding='utf-8') as f:
                for link in broken_links:
                    f.write(f"{link}\n")
            print(f"\nğŸ“ éœ€è¦ä¿®å¤çš„é“¾æ¥å·²ä¿å­˜åˆ°: broken_links.txt")

def main():
    checker = LinkChecker()
    print("å¼€å§‹æ£€æŸ¥ç½‘ç«™é“¾æ¥...")
    checker.check_all_links()
    print("\næ£€æŸ¥å®Œæˆ!")

if __name__ == "__main__":
    main()
